<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CMS HCAL</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="cmssw">CMSSW</h1>
<p>A quick summary: CMSSW is a C++ code that uses Thread Building Blocks (TBB) for multi-threading. Python is used but only for configuration files which tell the compiler which .so files to dynamically link to the base  executable <code>cmsRun</code>. You run the code with <code>cmsRun config.py</code>. All compiling and building are done using their <code>scram</code> tool. Scram, cmsRun, and all cmssw software releases are stored on a LHC file system called <code>cvmfs</code>, which supports the entire LHC project so its size is enormous ~ 50+ TB. Technically you could copy the few files you need for cmssw off of cvmfs but that won’t be necessary as cvmfs mounts on the Docker image you’ll be running. The input and output data files are ROOT which is a hierarchical data storage format with Python packages for reading and writing.</p>
<h1 id="useful-links">Useful Links</h1>
<p>From top to bottom, in order of importance</p>
<p><strong>CMS project</strong></p>
<ul>
<li><a href="https://github.com/cms-sw/cmssw/tree/CMSSW_7_5_X">CMS github</a></li>
<li><a href="https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBook">CMS “Workbook”</a></li>
<li><a href="https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideScram">scram</a> (their compile, build, and package manager tool)</li>
<li><a href="https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuide">CMS Software Developement Guid</a></li>
<li><a href="http://cdsweb.cern.ch/record/922757/files/lhcc-2006-001.pdf">Technical Document on the CMS experiment and software (500 pg)</a></li>
</ul>
<p><strong>NESAP</strong></p>
<ul>
<li><a href="https://drive.google.com/drive/u/0/folders/0AKaeC40GwofKUk9PVA?ogsrc=32">Ian Cosden’s VTune + Advisor slides</a>, if that link doesn’t work find <code>VTune_Advisor_Ian_Cosden_2018.pdf</code> in the NESAP team drive</li>
<li><a href="https://docs.google.com/presentation/d/1imZrdx6RaDVLKSkyaFq6VyzNhCharBW5iSUEhusLBDk/edit#slide=id.g3cd7237969_0_8vi">Jonathan Madsen’s VTune+Shifter slides</a></li>
<li><a href="https://software.intel.com/en-us/articles/optimization-and-performance-tuning-for-intel-xeon-phi-coprocessors-part-2-understanding">Intel docs on VTune</a></li>
</ul>
<h1 id="collaborators">Collaborators</h1>
<p>The head PIs are Dirk Hufnagel (<a href="mailto:hufnagel@fnal.gov">hufnagel@fnal.gov</a>) and Jim Kowalkowski (<a href="mailto:jbk@fnal.gov">jbk@fnal.gov</a>), but I primarily emailed and worked with Matti Kortelainen (<a href="mailto:matti.kortelainen@cern.ch">matti.kortelainen@cern.ch</a>). These were the guys to talk to for the Simulation HCAL project.</p>
<p>The Kalman filter project was Matti and a bunch of other CMS folks. They use this listserv for the project <code>"mic-trk-rd@cern.ch" &lt;mic-trk-rd@cern.ch&gt;</code>. Steve Lantz runs their weekly telecon (<a href="mailto:steve.lantz@cornell.edu">steve.lantz@cornell.edu</a>). The use <code>vidyo</code> for their telecons. Once you download it you can join the call with this link<br>
<a href="https://client-select.web.cern.ch/?url=https%3A%2F%2Fvidyoportal.cern.ch%2Fflex.html%3Froomdirect.html%26key%3DXex6rkoqz8716Tk2iPJo4h91E">https://client-select.web.cern.ch/?url=https%3A%2F%2Fvidyoportal.cern.ch%2Fflex.html%3Froomdirect.html%26key%3DXex6rkoqz8716Tk2iPJo4h91E</a></p>
<h1 id="projects">Projects</h1>
<p>We loosely defined two projects. A small self-contained one, envisioned as a warm up, <strong>Simulation HCAL</strong>, and a second one to replace current track reconstruction with a <strong>Kalman filter for reconstruction</strong></p>
<p>In terms of computing, there is <em>no need</em> to have a fermilab or cern account. Of course, you may need a fermilab and/or cern account to access their documentation, for formerly working on the projects as part of the cms collaboration, but don’t listen to the docs when they say you need a compute account.</p>
<p>The code for the Simulation HCAL project is here</p>
<p><strong>simulation HCAL</strong></p>
<ul>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/RecoLocalCalo/HcalRecProducers/src/HBHEPhase1Reconstructor.cc">https://github.com/cms-sw/cmssw/blob/master/RecoLocalCalo/HcalRecProducers/src/HBHEPhase1Reconstructor.cc</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/RecoLocalCalo/HcalRecAlgos/interface/MahiFit.h">https://github.com/cms-sw/cmssw/blob/master/RecoLocalCalo/HcalRecAlgos/interface/MahiFit.h</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/RecoLocalCalo/HcalRecAlgos/src/MahiFit.cc">https://github.com/cms-sw/cmssw/blob/master/RecoLocalCalo/HcalRecAlgos/src/MahiFit.cc</a></li>
</ul>
<p>Matti, also pointed me to each of these as separate smaller projects if interested</p>
<p><strong>pixel tracker</strong></p>
<ul>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiPixelDigitizer/plugins/SiPixelDigitizer.h">https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiPixelDigitizer/plugins/SiPixelDigitizer.h</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiPixelDigitizer/plugins/SiPixelDigitizer.cc">https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiPixelDigitizer/plugins/SiPixelDigitizer.cc</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiPixelDigitizer/plugins/SiPixelDigitizerAlgorithm.h">https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiPixelDigitizer/plugins/SiPixelDigitizerAlgorithm.h</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiPixelDigitizer/plugins/SiPixelDigitizerAlgorithm.cc">https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiPixelDigitizer/plugins/SiPixelDigitizerAlgorithm.cc</a></li>
</ul>
<p><strong>strip tracker</strong></p>
<ul>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiStripDigitizer/plugins/SiStripDigitizer.h">https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiStripDigitizer/plugins/SiStripDigitizer.h</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiStripDigitizer/plugins/SiStripDigitizer.cc">https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiStripDigitizer/plugins/SiStripDigitizer.cc</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiStripDigitizer/plugins/SiStripDigitizerAlgorithm.h">https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiStripDigitizer/plugins/SiStripDigitizerAlgorithm.h</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiStripDigitizer/plugins/SiStripDigitizerAlgorithm.cc">https://github.com/cms-sw/cmssw/blob/master/SimTracker/SiStripDigitizer/plugins/SiStripDigitizerAlgorithm.cc</a></li>
</ul>
<p><strong>ECAL</strong></p>
<ul>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/EcalSimProducers/interface/EcalDigiProducer.h">https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/EcalSimProducers/interface/EcalDigiProducer.h</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/EcalSimProducers/src/EcalDigiProducer.cc">https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/EcalSimProducers/src/EcalDigiProducer.cc</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/EcalSimAlgos/interface/EcalTDigitizer.h">https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/EcalSimAlgos/interface/EcalTDigitizer.h</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/EcalSimAlgos/interface/EcalTDigitizer.icc">https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/EcalSimAlgos/interface/EcalTDigitizer.icc</a></li>
</ul>
<p><strong>HCAL</strong></p>
<ul>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/HcalSimProducers/interface/HcalDigiProducer.h">https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/HcalSimProducers/interface/HcalDigiProducer.h</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/HcalSimProducers/src/HcalDigiProducer.cc">https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/HcalSimProducers/src/HcalDigiProducer.cc</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/HcalSimProducers/interface/HcalDigitizer.h">https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/HcalSimProducers/interface/HcalDigitizer.h</a></li>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/HcalSimProducers/src/HcalDigitizer.cc">https://github.com/cms-sw/cmssw/blob/master/SimCalorimetry/HcalSimProducers/src/HcalDigitizer.cc</a></li>
</ul>
<h1 id="shifter">Shifter</h1>
<p>Grab the CMS docker image.</p>
<pre><code>ssh &lt;user&gt;@cori.nersc.gov
shifterimg -v pull docker:bbockelm/cms:rhel6
</code></pre>
<p>Run shifter from login node</p>
<pre><code>shifter --image=bbockelm/cms:rhel6 --module=cvmfs /bin/bash
</code></pre>
<p>or as an interactive job.</p>
<pre><code># Haswell
salloc -N 1 -C haswell -q interactive --image=bbockelm/cms:rhel6 --module=cvmfs -t 00:30:00
# KNL
salloc -N 1 -C knl -q interactive --image=bbockelm/cms:rhel6 --module=cvmfs -t 00:30:00
# then run shifter
shifter --image=bbockelm/cms:rhel6 --module=cvmfs /bin/bash
</code></pre>
<h1 id="create-project-hcal-and-compile-it">Create project “HCAL” and Compile it</h1>
<p>HCAL is a standalone simulation reconstruction module. CMS creates “project directories” and we are forced to develop and run code out of it.</p>
<pre><code>shifter --image=bbockelm/cms:rhel6 --module=cvmfs /bin/bash
source /cvmfs/cms.cern.ch/cmsset_default.sh
#alias cmsenv='eval `scramv1 runtime -sh`'
#alias cmsrel='scramv1 project CMSSW’
export SCRAM_ARCH=slc6_amd64_gcc630
#cd &lt;workdir&gt;
cd $CSCRATCH/cms
scram project CMSSW_10_1_0
#for versions besides 10_1_0 do
#ls /cvmfs/cms.cern.ch/slc6_amd64_gcc630/cms/cmssw
</code></pre>
<p>The directory CMSSW_10_0/src is empty. Put the handful of modules from github/cmssw for HCAL in CMSSW_10_0/src using the CMS wrapped git command “git cms-addpkg”. Then run scram b from src/ to build the .so files.</p>
<pre><code>cd CMSSW_10_1_0/src
#equivalent to: eval `scramv1 runtime -sh`
cmsenv
git cms-addpkg RecoLocalCalo/HcalRecProducers
git cms-addpkg RecoLocalCalo/HcalRecAlgos
#git cms-addpkg DataFormats/Pat
git cms-addpkg DataFormats
</code></pre>
<h2 id="profilers">Profilers</h2>
<p>This document shows how to use two profiling tools with the Simulation HCAL project.</p>
<ul>
<li>Intel VTune: hotspots, OpenMP scaling, Cache, Hardware</li>
<li>Intel Adviser: vectorization</li>
</ul>
<p>Ian Cosden (CMS HPC performance, Princeton) has worked with each of these extensively so would be a good person to collaborate with.</p>
<h2 id="compile-for-vtune.">Compile for VTune.</h2>
<p>VTune requires that you compile your code with debug symbols, e.g. with the “-g” option. You can edit all of <code>scram</code>'s compilation settings  by editing this .xml file located in the directory you created with <code>scram project &lt;name&gt;</code>. e.g.,  <code>$CSCRATCH/cms/CMSSW_10_1_0/config/toolbox/slc6_amd64_gcc630/tools/selected/gcc-cxxcompiler.xml</code>. Do the following</p>
<ul>
<li>Add <code>-g</code> to <code>CXXFLAGS</code></li>
<li>cd $CSCRATCH/cms/CMSSW_10_1_0/src</li>
<li>scram setup gcc-cxxcompiler</li>
<li>scram b clean</li>
<li>scram b -v &amp;&gt; compile_output.txt</li>
</ul>
<p>Note, the -v prints out all the compilation commands, e.g. so you can check that it compiles how you want for VTune. Also, cms builds with <code>-O2</code> by default you might try also <code>-O3</code>,</p>
<h4 id="i-found-that-these-compile-options-did-not-help">I found that these compile options did not help</h4>
<ul>
<li>Adding <code>-g -ftree-vectorizer-verbose=7</code>, see <a href="https://monoinfinito.wordpress.com/series/vectorization-in-gcc/">here</a>,  to print suggestions for vectorizing loops to the output of scram b</li>
<li>Add  <code>-debug</code> for inline-debug-info, add b/c the compiler may optimize this away, recommended by Ian Cosden (CMS HPC performance, Princeton)<br>
scram b -v &amp;&gt; compile_output.txt</li>
</ul>
<h4 id="i-didnt-try-these-so-might-be-worthwhile">I didn’t try these, so might be worthwhile</h4>
<ul>
<li><code>-O3</code> instead of <code>-O2</code>
<ul>
<li>Matti recalls from ~8 years ago  that different -Os gave improvement, mainly because (at least with the CPUs of that time) they had an issue with the instruction cache. The CPU spent time idle waiting instructions from the caches+memory because the code is so big.</li>
</ul>
</li>
</ul>
<h2 id="compile-for-advisor">Compile for Advisor</h2>
<p>Ian Cosden recommended compiling with <code>-g -dynamic</code> but <code>gcc</code> does not recognize <code>-dynamic</code>, so that must be an <code>icc</code> thing</p>
<p>Advisor does not have the same <code>target-pid</code> command line option that VTune has, so Jonathan’s solution for running VTune on a Docker image does <strong>not</strong> apply to Advisor. is the NERSC Intel contact for VTune. I emailed her and cc’d Rollin asking about whether such an option exists for Advisor. I didn’t hear back.</p>
<h2 id="warning-aliases">Warning: aliases</h2>
<p>CMS runs a shifter job by executing a script with shifter, then their cmsRun executable gets called in that script. Something like</p>
<pre><code>shifter script.sh

# script.sh
source /cvmfs/cms.cern.ch/cmsset_default.sh
...
cmsRun config.py
</code></pre>
<p>However, any aliases defined in <code>/cvmfs/cms.cern.ch/cmsset_default.sh</code> are not preserved (possibly unintended consequence of shifter) so “cmsenv” and “cmsrel” may not be defined.</p>
<h2 id="create-the-python-config-file-for-hcal">Create the Python config file for HCAL</h2>
<p>Matti suggested running <a href="https://github.com/cms-sw/cmssw/blob/CMSSW_7_5_X/Configuration/PyReleaseValidation/scripts/runTheMatrix.py">runTheMatrix.py</a> which uses <a href="https://github.com/cms-sw/cmssw/blob/CMSSW_7_5_X/Configuration/Applications/scripts/cmsDriver.py">cmsDriver.py</a> to generate a series of config files (<a href="http://step1.py">step1.py</a>, <a href="http://step2.py">step2.py</a>, …) related to HCAL. This is better than running <code>cmsDriver.py</code> directly, as the CMS Workbook says to do, because each step can be run as a separate compute tasks. Step3 is HCAL. cmsDriver would merge them all into a single config file. You do <code>cmsRun stepX.py</code> for each config file.</p>
<pre><code>shifter --image=bbockelm/cms:rhel6 --module=cvmfs /bin/bash
cd $CSCRATCH/cms
git clone https://github.com/cms-sw/cmssw
cd cmssw
python Configuration/PyReleaseValidation/scripts/runTheMatrix.py -l 10824.0 -j 0
</code></pre>
<p>this makes five config files, HCAL uses the third one. The names of these files are crazy long so rename them</p>
<pre><code>cd $CSCRATCH/cms
mv cmssw/10824.0_TTbar_13+TTbar_13TeV_TuneCUETP8M1_2018_GenSimFull+DigiFull_2018+RecoFull_2018+ALCAFull_2018+HARVESTFull_2018 hcal_config
#all .py files except this one has prefix step
#assume this one is step1
mv hcal_config/TTbar_13TeV_TuneCUETP8M1_cfi_GEN_SIM.py hcal_config/step1_cfi_GEN_SIM.py
</code></pre>
<p>Now you’ll submit the following batch jobs to run steps 1-3. Running each step creates the output ROOT file that the next steps reads as input. Step 3 is the HCAL simulation.</p>
<pre><code>cd $CSCRATCH/cms/rundir
# edit job_cms.slurm and job_cms.sh to run "step1.py"
# cmsRun $config_dir/step1_cfi_GEN_SIM.py
sbatch job_cms.sh
# edit for step 2
# cmsRun $config_dir/step2_DIGI_L1_DIGI2RAW_HLT.py
sbatch job_cms.sh
# edit for step 2
# cmsRun $config_dir/step3_RAW2DIGI_L1Reco_RECO_RECOSIM_EI_PAT_VALIDATION_DQM.py
sbatch job_cms.sh
</code></pre>
<p>Now that you can run step 3 to completion, let’s hook it up to VTune</p>
<h2 id="shifter-interactive">Shifter Interactive</h2>
<p>A quick note that once you’ve make your project dir with <code>scram project &lt;name&gt;</code>, always being an interactive shifter job by copy and pasting the following</p>
<pre><code>shifter --image=bbockelm/cms:rhel6 --module=cvmfs /bin/bash
source /cvmfs/cms.cern.ch/cmsset_default.sh 
export SCRAM_ARCH=slc6_amd64_gcc630 
cd $CSCRATCH/cms/CMSSW_10_1_0/src
cmsenv
#eval `scramv1 runtime -sh`
</code></pre>
<h2 id="vtune">VTune</h2>
<p>VTune is “shared memory only” so the code you are testing must fall in one of these categories: Serial, OpenMP, or MPI on single node</p>
<p>I got step 2 working before 3, so I’ll test out VTune on step 2 for now. Jonathon Madsen figured it out: submit the Shifter job as a background job then run VTune setting <code>--target-pid=&lt;PID-of-job&gt;</code>.</p>
<pre><code>ssh &lt;user&gt;@cori.nersc.gov
mkdir $CSCRATCH/cms/rundir/vt
cd vt
module load vtune/2018.up2
sbatch job_cms.slurm

# job_cms.slurm
...
#SBATCH --perf=vtune/2018.up2
...
PID_FILE=$(mktemp pid.XXXXXX)
srun -n 1 -c 68 --cpu_bind=cores shifter $rundir/job_cms.sh &amp; echo $! &amp;&gt; ${PID_FILE}
TARGET_PID=$(cat ${PID_FILE})
amplxe-cl -collect general-exploration -r . -trace-mpi --target-pid=${TARGET_PID}
</code></pre>
<p>Now lets analyze the output with the VTune GUI. Using NX,</p>
<pre><code>cd $CSCRATCH/cms/rundir/vt/vtout.nid02518
module load vtune/2018.up2
amplxe-gui &amp;
</code></pre>
<h2 id="what-i-learned-about-vtune">What I learned about VTune</h2>
<ul>
<li>don’t use <code>srun</code> and <code>shifter</code> in the same command because VTune will profile <code>srun</code></li>
<li>use one dash <code>-target-pid</code> not two dashes <code>--target-pid</code></li>
<li><code>-search-dir</code> provides no additional info to VTune + docker when calling amplexi</li>
<li>don’t use <code>-tracempi</code> if using 1 Node</li>
<li>CMS uses gcc, have they ever tried intel?</li>
<li>After Googling, there does not appear to be additional compile options besides <code>-g</code> when using <code>gcc</code> and <code>VTune</code>. The worry was that <code>icc + VTune</code> gives better output that <code>gcc + VTune</code> but that does not sound like it’s the case</li>
<li>gcc versus icc: Matti said, IIRC the Intel compiler has been tested years ago. I don’t remember its impact on the performance, but I do recall us having code for which gcc and icc interpret the C++ standard differently. I can also think of bunch of other issues, but for a demonstrated performance improvement I suppose we (CMS) can reconsider.</li>
<li>which VTune <code>-collector</code>should you use?
<ul>
<li><code>general-exploration</code>, <code>hotspots</code>, <code>hpc-performance</code> are plenty</li>
<li><code>memory-consumption</code>, <code>disk-io</code> failed</li>
<li><code>memory-access</code> added nothing</li>
<li><code>disk-io</code> results in the following error from VTune at runtime, google and the postdocs didn’t know how to compile code to enable Ftrace</li>
</ul>
</li>
</ul>
<pre><code>amplxe: Warning: Kernel function tracing via ftrace is not available. I/O wait thread state is determenied heuristically. Please recompile the kernel with CONFIG_FUNCTION_TRACER enabled for better results.
amplxe: Error: Ftrace collection is not possible due to a lack of credentials. Make sure you have read/write access to debugFS. You may either run the analysis with root privileges (recommended) or follow the configuration instructions provided in the Software Event Library help topic.
- which VTune GUI `grouping` should you use?
	- See section below, "Results from VTune"

</code></pre>
<h2 id="results-from-vtune-for-config-step2.py">Results from VTune (for config <a href="http://step2.py">step2.py</a>)</h2>
<p>The bottlenecks tend to be .so files where info on the bottleneck function or line number is not available; however, when you do have the option to looking at the source code with VTune the files cannot be found because they are on the Docker image.</p>
<p>the main summary is<br>
General Exploration/Summary<br>
<img src="https://lh3.googleusercontent.com/Su0__bjNCF597kAizz5owU6VM8_QgIT7bERRrbHYkHd4zWIgGYiaI3W8U69W4UNTrXhtzwukRjde" alt="enter image description here"></p>
<p>Of note is pthreads = 31 which means that a KNL job using all 68 cores will probably perform as well as the same job with only 31 cores</p>
<p>General Exploration/Bottom-up<br>
bottom right "functions only"<br>
This shows only .so files and functions. The .so files dominate the cpu time. The top few are:</p>
<ul>
<li><a href="http://libFWCoreFramework.so">libFWCoreFramework.so</a></li>
<li><a href="http://ld.so">ld.so</a>, linux dynamical object linker</li>
<li><a href="http://libCling.so">libCling.so</a>, c++ library for ROOT data types</li>
<li><a href="http://libpython2.7.so">libpython2.7.so</a><br>
<img src="https://lh3.googleusercontent.com/lkmXk2plEwYD0N9cOQJ_XN73H1z7ug3Oh8jtI3wI892iVPkVXcTO25CXtZ-PHJsiOTZLq_lxltMm" alt="enter image description here"></li>
</ul>
<p>None of these besides <a href="http://python2.7.so">python2.7.so</a> can be improved. However, I remember reading and hearing about speedups of 10-25% by people simply using python3 instead of python2</p>
<p>HPC Performance Characteristics/Bottom-up<br>
enlarge panel on far right.<br>
<img src="https://lh3.googleusercontent.com/jP2HAhPYxaZQX-2L5DWX3HFopvvlvAXWebairlqAZej1h3N3Iupgp2XFxmgsCJ1NYPNh2lzhbI80" alt="enter image description here"><br>
Key numbers are</p>
<ul>
<li>L2 Hit 7%, Miss 8%</li>
<li>SIMD packed 40%, Scalar 60%</li>
</ul>
<p>L2 Hit is the fraction of cycles spent on getting data from L2 cache, while Miss is the fraction of cycles used getting data when it isn’t found on L2. Is 5% small?</p>
<p>SIMD Instr is how packed the 512-bit vector processing units are. e.g. they are 40% and 60% full. Vectorization should help the code.</p>
<p>Now, lets look for costly loops. Set “grouping” to <code>Source Function/Function/Call Stack</code> and go to<br>
General Exploration/Hotspots<br>
on bottom right select loops only:<br>
<img src="https://lh3.googleusercontent.com/_F8xYATypBnocwZrIwVnQno5_Z6KBZ1LnGhra8QzSQsLKBjEcE7mv_pIXR5Nc5X4h7jZmtPLO3bw" alt="enter image description here"></p>
<ul>
<li>but 99.9% of the time apparently occurs outside any loop</li>
</ul>
<p>General Exploration/Hotspots<br>
on bottom right select "functions &amp; loops"<br>
then pull up lower panel<br>
<img src="https://lh3.googleusercontent.com/7IZLyL-_zhD3xwrc001mgxQvsNOSQysDs_-Ix2HTCij_PJgN35UWVau3gwJuQIpZYverlC4MtAMy" alt="enter image description here"><br>
brown: good, orange: bad, green: idle</p>
<ul>
<li>agrees with above, all but 1 thread is doing work, one module or file consuming all the time</li>
</ul>
<h2 id="intel-adviser----vectorization-tuning">Intel Adviser --&gt; vectorization tuning</h2>
<p>Ian Cosden’s recommended tuner for vectorization in CMS.</p>
<h1 id="conclusions">Conclusions</h1>
<p>There scripts on my github, as shown above, run VTune on your own compiled cms module and the VTune output is complete enough that you can look at individual files and specific line numbers for bottlenecks. However, the fundamental problem is that 99% of the time is spent in the cms FrameWork .so file called <code>libFWCoreFramework.so</code>, which VTune cannot see into.</p>
<p>For example,<br>
General Exploration/Bottom-up<br>
bottom right “functions only” <img src="https://lh3.googleusercontent.com/lkmXk2plEwYD0N9cOQJ_XN73H1z7ug3Oh8jtI3wI892iVPkVXcTO25CXtZ-PHJsiOTZLq_lxltMm" alt="enter image description here"></p>
<p>Rollin and I think that the <code>cmsRun</code> executable is not compiled with the <code>-g</code> debug symbols flag, so VTune cannot profile <code>libFWCoreFramework.so</code>. We are “supposed” to use the pre-built <code>cmsRun</code> stored in the <code>cvmfs</code> module, but the next step in this project is the figure out how to build <code>cmsRun</code> ourselves with <code>-g</code>. I asked Matti about this a few weeks ago and everyone simply uses <code>cmsRun</code> and doesn’t build it. So another expert is probably needed for this part. Maybe Wahid or Lisa know enough to team up to do this.</p>
<p>The code for cmsRun is here</p>
<ul>
<li><a href="https://github.com/cms-sw/cmssw/blob/master/FWCore/Framework/bin/cmsRun.cpp">https://github.com/cms-sw/cmssw/blob/master/FWCore/Framework/bin/cmsRun.cpp</a></li>
</ul>
<p>Beyond that, figuring out how to run Advisor on a Docker image (remember, <code>-target-pid</code> is not an option in Advisor so Jonathon’s VTune + Docker solution does not work) is the next step for improving vectorization.  And it looks like vectorization would improve things by a factor of ~ 2 as the SIMD instructions are half packed.</p>
<p>For example,<br>
HPC Performance Characteristics/Bottom-up<br>
enlarge panel on far right.<br>
<img src="https://lh3.googleusercontent.com/jP2HAhPYxaZQX-2L5DWX3HFopvvlvAXWebairlqAZej1h3N3Iupgp2XFxmgsCJ1NYPNh2lzhbI80" alt="enter image description here"></p>
</div>
</body>

</html>
